{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "from utils import set_random, compute_entropy_v2\n",
    "from training_utils import FocalLoss, validate_v2\n",
    "from model import UNet\n",
    "from model_v2 import deeplabv3_resnet50\n",
    "from model_v3 import MC_DeepLab\n",
    "from model_v4 import create_ViT_model\n",
    "from create_dataset import (\n",
    "    get_datasets,\n",
    "    get_image_curr_labeled,\n",
    ")\n",
    "from routes import (\n",
    "    CONTINUE_PATH,\n",
    "    CONTINUE_FOLDER,\n",
    "    PRINT_PATH,\n",
    "    IGNORE_INDEX\n",
    ")\n",
    "\n",
    "#from other_queries import (\n",
    "#    GT_query,\n",
    "#    GTxSim_query,\n",
    "#    density_entropy_query,\n",
    "#    density_classEntropyV2_query,\n",
    "#    density_query,\n",
    "#    entropy_query,\n",
    "#    BvSB_patch_query,\n",
    "#    class_entropy_query,\n",
    "#    class_entropy_patch_query,\n",
    "#    class_entropy_ML_pred_patch_query,\n",
    "#    class_entropy_video_query,\n",
    "#    random_query,\n",
    "#    coreset_query,\n",
    "#    coreset_entropy_query,\n",
    "#    COWAL_center_query,\n",
    "#    k_means_entropy_query,\n",
    "#    COWAL_entropy_query,\n",
    "#    COWAL_entropy_video_query,\n",
    "#    COWAL_entropy_patch_query,\n",
    "#    MC_dropout_query,\n",
    "#    BALD_query,\n",
    "#    suggestive_annotation_query,\n",
    "#   suggestive_annotation_patch_query,\n",
    "#    VAAL_query,\n",
    "#    oracle_query,\n",
    "#    COWAL_classEntropy_query,\n",
    "#    COWAL_classEntropy_video_query,\n",
    "#    COWAL_classEntropy_v2_query,\n",
    "#    COWAL_classEntropy_v2_video_query,\n",
    "#    COWAL_classEntropy_v2_1_query,\n",
    "#    COWAL_classEntropy_v2_2_query,\n",
    "#    COWAL_classEntropy_v2_2_video_query,\n",
    "#    COWAL_classEntropy_patch_query,\n",
    "#    COWAL_classEntropy_patch_v2_query,\n",
    "#    RIPU_PA_query,\n",
    "#    entropy_patch_query,\n",
    "#    revisiting_query,\n",
    "#    revisiting_v2_query,\n",
    "##    revisiting_adaptiveSP_query,\n",
    "#    CBAL_query,\n",
    "#    CBAL_v2_query,\n",
    "#    pixelBal_query,\n",
    "#    pixelBal_v2_query,\n",
    "#    BvSB_patch_v2_query,\n",
    "#)\n",
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train video: ['01-00', '01-01', '01-03', '01-04', '01-07', '02-00', '02-01', '03-00', '03-01', '03-03', '03-04', '03-06', '03-07', '04-04', '04-07', '10-03', '11-01', '12-03', '13-02', '13-03', '15-00', '15-04', '15-05'], 23\n",
      "val video: ['01-02', '01-06', '03-02', '03-05', '04-06', '08-02', '10-01', '13-00', '15-01', '15-02'], 10\n",
      "test video: ['08-00', '08-01', '08-03', '08-04', '08-05', '08-06', '08-07', '08-08', '08-09', '09-00', '09-01', '09-02', '10-00', '10-02', '10-05', '10-06', '10-07', '11-00', '11-02', '11-03', '11-04', '13-04'], 22\n",
      "\n",
      "sampling is: random\n",
      "number of labeled train frames: 528 - 720 patches - 20.0 whole images\n",
      "number of train frames: 1098 - total patches 39528\n",
      "number of val frames: 629\n",
      "number of test frames: 1148\n",
      "number of unlabeled frames: 1098 - 38808 patches - 1078.0 whole images\n",
      "first dataset sample: 15-00/frame4119.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "set_random(SEED)\n",
    "\n",
    "### SPLIT TRAIN SET ###\n",
    "curr_labeled, curr_selected_patches, train_data, val_data, test_data, TRAIN_SEQ, VAL_SEQ, TEST_SEQ = get_image_curr_labeled(config, SEED, notebook=True)\n",
    "\n",
    "################### define patience iter ###################\n",
    "if config['N_LABEL'] > 1 and config['DATASET'] not in ['auris', 'intuitive', 'cityscapes', 'pascal_VOC'] and not config['BALANCED_INIT']:                                                 \n",
    "    config['PATIENCE_ITER'] = 2 * (config['INIT_NUM_VIDEO'] + (config['NUM_ROUND'] - 1) * config['NUM_QUERY'])\n",
    "# # elif config['N_LABEL'] > 1 and config['DATASET'] != 'auris' and config['BALANCED_INIT']:\n",
    "#     config['PATIENCE_ITER'] = 2 * ((config['N_LABEL'] - 1) * 2 + (config['NUM_ROUND'] - 1) * config['NUM_QUERY'])\n",
    "config[\"PATIENCE_ITER\"] = config[\"PATIENCE_ITER\"] // config[\"BATCH_SIZE\"]\n",
    "\n",
    "if config['DATASET'] == 'auris': # auris v11\n",
    "    auris_patience_iter = [132, 90, 130, 130, 130, 127, 142, 113, 113, 113]\n",
    "    config['PATIENCE_ITER'] = auris_patience_iter[SEED]\n",
    "##################### \n",
    "    \n",
    "start_round = 0\n",
    "test_scores = []\n",
    "\n",
    "\n",
    "set_random(SEED)  # to have the same model parameters\n",
    "\n",
    "### SET UP DATASET ###\n",
    "(\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    test_dataloader,\n",
    "    all_train_dataset,\n",
    "    train_dataset_noAug,\n",
    "    unlabeled_dataset,\n",
    ") = get_datasets(\n",
    "    config,\n",
    "    curr_labeled,\n",
    "    curr_selected_patches=curr_selected_patches,\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    TRAIN_SEQ=TRAIN_SEQ,\n",
    "    VAL_SEQ=VAL_SEQ,\n",
    "    TEST_SEQ=TEST_SEQ,\n",
    "    notebook=True,\n",
    ")\n",
    "if config[\"NUM_ROUND\"] == 1:\n",
    "    config[\"PATIENCE_ITER\"] = len(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/feiwu/opt/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/Users/feiwu/opt/anaconda3/envs/torch/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "n_round = 0\n",
    "model = deeplabv3_resnet50(\n",
    "    pretrained=False, num_classes=config[\"N_LABEL\"]\n",
    ").to(config[\"DEVICE\"])\n",
    "\n",
    "copy_model = copy.deepcopy(model).to(config[\"DEVICE\"])\n",
    "copy_model.eval()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"LEARNING_RATE\"])\n",
    "criterion = nn.BCEWithLogitsLoss() if config['N_LABEL'] == 1 else FocalLoss()\n",
    "\n",
    "start_epoch = 0\n",
    "best_val_score = 0\n",
    "best_val_class_dices = {}\n",
    "patience = 0\n",
    "curr_iter = 0\n",
    "nb_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN THE MODEL ###\n",
    "\n",
    "image, mask, n = next(iter(train_dataloader))\n",
    "image, mask = image.to(config[\"DEVICE\"]), mask.to(config[\"DEVICE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 220, 220])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 220, 220])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/m0/g6bzt3kn1rx_ss5hyj4yhxkr0000gn/T/ipykernel_1405/713670937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __call__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "# optimizer.zero_grad()\n",
    "outputs = model(image)\n",
    "loss = criterion(outputs, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 220, 220])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 220, 220])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
